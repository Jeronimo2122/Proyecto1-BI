{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proyecto: Turismo de los Alpes  - Grupo 34\n",
    "Felipe Nuñez\n",
    "<br>\n",
    "Jeronimo Vargas Rendon - 202113305\n",
    "<br>\n",
    "Estudiante 1: Juan Manuel Pérez - 202021827"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Jeronimo\n",
      "[nltk_data]     Vargas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importar librerias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de datos\n",
    "df = pd.read_csv('tipo1_entrenamiento_estudiantes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Entendimiento de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  Class\n",
      "0  Nos alojamos en una casa alquilada en la ciuda...      4\n",
      "1  La comida está bien, pero nada especial. Yo te...      3\n",
      "2  En mi opinión, no es una como muchos usuarios ...      3\n",
      "3  esta curiosa forma que asemeja una silla de mo...      4\n",
      "4  Lo mejor era la limonada. Me gusto la comida d...      2\n",
      "Total de registros: 7875\n",
      "Review    0\n",
      "Class     0\n",
      "dtype: int64\n",
      "Duplicados: 71\n"
     ]
    }
   ],
   "source": [
    "# Visualización de los primeros registros\n",
    "print(df.head())\n",
    "\n",
    "# Total de registros\n",
    "print(\"Total de registros:\", df.shape[0])\n",
    "\n",
    "# Verificación de valores faltantes\n",
    "print(df.isnull().sum())\n",
    "\n",
    "#Contar duplicados\n",
    "print(\"Duplicados:\" , df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Review', 'Class'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Imprimir df\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar duplicados\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_noise(text):\n",
    "    text = re.sub(r'<.*?>', '', text)  # Eliminar HTML tags\n",
    "    text = re.sub(r'[0-9]+', '', text)  # Eliminar números\n",
    "    text = re.sub(r'[^a-záéíóúñA-ZÁÉÍÓÚÑ\\s]', '', text)  # Eliminar caracteres especiales\n",
    "    return text\n",
    "\n",
    "df['Review'] = df['Review'].apply(remove_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Jeronimo\n",
      "[nltk_data]     Vargas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Jeronimo\n",
      "[nltk_data]     Vargas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Descarga de los recursos necesarios de NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Definición de las stopwords en español\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "\n",
    "def clean_text(text):\n",
    "    # Tokenización del texto\n",
    "    words = word_tokenize(text.lower())\n",
    "    # Filtrado de palabras que no son stopwords y son alfabéticas\n",
    "    filtered_words = [word for word in words if word not in stop_words and word.isalpha()]\n",
    "    # Reconstrucción del texto\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "# Asumiendo que 'Review' es la columna con los textos a limpiar\n",
    "df['Review'] = df['Review'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jeronimo Vargas\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'es_core_news_sm' (3.1.0) was trained with spaCy v3.1.0 and may not be 100% compatible with the current version (3.7.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import spacy # type: ignore\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "def normalize_text(text):\n",
    "    doc = nlp(text.lower())\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    return ' '.join(lemmas)\n",
    "df['Review'] = df['Review'].apply(normalize_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para preprocesar el texto\n",
    "def preprocess_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = text.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word.lower()) for word in words if word.lower() not in stopwords.words('english')]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "\n",
    "# Aplicación de la función de preprocesamiento\n",
    "df['processed_text'] = df['Review'].apply(preprocess_text)\n",
    "\n",
    "# Vectorización\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(df['processed_text']).toarray()\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))  # Utiliza bigramas además de unigramas\n",
    "X = vectorizer.fit_transform(df['Review'])\n",
    "\"\"\"\n",
    "\n",
    "# Extracción etiquetas\n",
    "y = df['Class']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.51      0.39      0.44       160\n",
      "           2       0.43      0.34      0.38       240\n",
      "           3       0.39      0.34      0.36       312\n",
      "           4       0.38      0.41      0.39       397\n",
      "           5       0.57      0.68      0.62       452\n",
      "\n",
      "    accuracy                           0.46      1561\n",
      "   macro avg       0.45      0.43      0.44      1561\n",
      "weighted avg       0.46      0.46      0.45      1561\n",
      "\n",
      "Accuracy: 0.4612427930813581\n"
     ]
    }
   ],
   "source": [
    "# Regression logistica\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "modelLR = LogisticRegression()\n",
    "modelLR.fit(X_train, y_train)\n",
    "y_pred = modelLR.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.56      0.24      0.33       160\n",
      "           2       0.41      0.38      0.39       240\n",
      "           3       0.38      0.28      0.32       312\n",
      "           4       0.38      0.40      0.39       397\n",
      "           5       0.53      0.74      0.62       452\n",
      "\n",
      "    accuracy                           0.45      1561\n",
      "   macro avg       0.45      0.41      0.41      1561\n",
      "weighted avg       0.45      0.45      0.44      1561\n",
      "\n",
      "Accuracy: 0.4535554131966688\n"
     ]
    }
   ],
   "source": [
    "# Modelo multinomial Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "modelNB = MultinomialNB()\n",
    "modelNB.fit(X_train, y_train)\n",
    "y_pred = modelNB.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.46      0.19      0.27       160\n",
      "           2       0.42      0.43      0.42       240\n",
      "           3       0.38      0.29      0.33       312\n",
      "           4       0.39      0.38      0.39       397\n",
      "           5       0.53      0.72      0.61       452\n",
      "\n",
      "    accuracy                           0.45      1561\n",
      "   macro avg       0.44      0.40      0.40      1561\n",
      "weighted avg       0.44      0.45      0.43      1561\n",
      "\n",
      "Accuracy: 0.4509929532351057\n"
     ]
    }
   ],
   "source": [
    "# Modelo Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "modelRF = RandomForestClassifier()\n",
    "modelRF.fit(X_train, y_train)\n",
    "y_pred = modelRF.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicciones Archivo de Prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  Class\n",
      "0  primero noche encontrar habitación nido cucara...      1\n",
      "1  calle catedral platillo tradicional tipo gourm...      4\n",
      "2  porción miserable agua sabor cloro distraído m...      2\n",
      "3  cartagena encantar todo ciudad colonial visita...      5\n",
      "4  ibar mucho ilusion disfrutar espectaculo luz s...      3\n"
     ]
    }
   ],
   "source": [
    "# Predicciones con el modelo a los siguientes Datos usando el mejor modelo LR\n",
    "\n",
    "df_pred = pd.read_csv('particion_prueba_estudiantes.csv')\n",
    "\n",
    "df_p = df_pred.copy()\n",
    "\n",
    "# Predecir con el modleo de Liner Regression\n",
    "df_p['Review'] = df_p['Review'].apply(remove_noise)\n",
    "df_p['Review'] = df_p['Review'].apply(clean_text)\n",
    "df_p['Review'] = df_p['Review'].apply(normalize_text)\n",
    "df_p['Review'] = df_p['Review'].apply(preprocess_text)\n",
    "X_pred = vectorizer.transform(df_p['Review']).toarray()\n",
    "y_pred = modelLR.predict(X_pred)\n",
    "df_p['Class'] = y_pred\n",
    "\n",
    "# Guardar predicciones\n",
    "df_pred['Class'] = y_pred\n",
    "df_pred.to_csv('predicciones_estudiantes.csv', index=False)\n",
    "print(df_p.head())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Resultados"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
